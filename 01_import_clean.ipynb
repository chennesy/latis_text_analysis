{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings\n",
    "To work with text in Python it's important to be able to manipulate string variables. Let's create a variable called text_string and print it as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there\n"
     ]
    }
   ],
   "source": [
    "text_string = 'Hi there'\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what kind of a variable this is by using the built-in type() function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to specific characters in the string by slicing it using brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n"
     ]
    }
   ],
   "source": [
    "print(text_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_string[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files\n",
    "\n",
    "Usually you want to work with text from files though, and not manually create string variables. \n",
    "\n",
    "The first step is to read in the files containing the data. Common file types for text data are: \n",
    "* `.txt`\n",
    "* `.csv`\n",
    "* `.json`\n",
    "* `.html` \n",
    "* `.xml`\n",
    "\n",
    "Each file format requires specific Python tools or methods to read, but for our case, we'll be working with .txt files.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "Let's take a look at the first file in our directory (folder) of State of the Union addresses (`/sotu_text`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable called file1 and read (\"r\") the first file in the sotu_text folder\n",
    "file1 = open(\"sotu_text/215.txt\",\"r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.TextIOWrapper"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but when we print the variable, it's not yet stored as a string\n",
    "type(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view the text, let's read in the file1 object to a new variable called \"text\" using .read() and then print out the first250 characters\n",
    "text = file1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. President, Mr. Speaker, Members of the 104th Congress, my fellow Americans: Again we are here in the sanctuary of democracy, and once again our democracy has spoken. So let me begin by congratulating all of you here in the 104th Congress and cong\n"
     ]
    }
   ],
   "source": [
    "print(text[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Once we've read in the data, a common next step is to split a longer string into words. This step is referred to as \"tokenization\". That's because each occurrence of a word is called a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text.\n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "Let's save each word to a list variable called 'tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the split() function to split the text variable up by whitespace into a tokens list\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what kind of a variable is tokens?\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists \n",
    "You can view each item in a Python list using the same syntax we used above to slice a str variable. The first item in the tokens list is at ```tokens[0]``` and the second is ```tokens[1]```. You can view a range of the first 10 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'President,',\n",
       " 'Mr.',\n",
       " 'Speaker,',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'the',\n",
       " '104th',\n",
       " 'Congress,',\n",
       " 'my']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first one\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['still', 'to', 'come.', 'Thank', 'you,', 'and', 'God', 'bless', 'you', 'all.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the last ten\n",
    "tokens[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Presi'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: you can also slice the string variables stored inside of a list\n",
    "tokens[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences, and provides a different way to tokenize our text.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instead of the default whitespace for split(), you can identify the character or characters you'd like to split on\n",
    "sentences = text.split('.')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many items are in any list using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What are we to do about it? \\n\\nMore than 60 years ago, at the dawn of another new era, President Roosevelt told our Nation, \"New conditions impose new requirements on Government and those who conduct Government'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that this method doesn't break out sentences that end with other punctuation, like question marks\n",
    "sentences[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "We could improve on this by using regular expressions. They allow us to split strings using specific characters or patterns that match different *kinds* of characters. Regex is a very powerful tool, but we won't go into it much today. For help figuring out and working with regular expressions we recommend https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pattern matches periods, question marks, or exclamation marks\n",
    "boundary_pattern = r'[.?!]'\n",
    "sentences_re = re.split(boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are now a few more sentences in our list\n",
    "len(sentences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' What are we to do about it'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and this sentence ends at the question mark\n",
    "sentences_re[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip whitespace\n",
    "\n",
    "This is an extremely common step in text cleaning. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hi there! '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \" Hi there! \"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi there!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ```strip()``` to remove \"line breaks\" from strings. Line breaks are often represented with \"escape characters\" such as ```\\n``` in text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and God bless you all.\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u, and God bless you all.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can remove whitespace at the beginning and end of a string using .strip()\n",
    "stripped_text = text.strip()\n",
    "stripped_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run more complex find/replace patterns using regex. Here we use ```re.sub()``` to match any \\s+ characters with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and God bless you all. '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use regular expressions to remove whitespace throughout the string\n",
    "# note that we are replacing any of the matching whitespace patterns with a single space ' '.\n",
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "Text normalization can help us clean our text to fit some standard patterns. One common normalization step is to remove (upper) case from the text.\n",
    "\n",
    "If you want to count the frequencies of words, for example, using lower case will ensure you don't count \"Death\" and \"death\" as two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi there! can you believe it's 2020?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps_string = \"Hi There! Can you believe it's 2020?\"\n",
    "caps_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mr. president, mr. speaker, members of the 104th congress, my fellow americans: again we are here in the sanctuary of democracy, and once again our democracy has spoken. so let me begin by congratulating all of you here in the 104th congress and cong'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text[0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your analysis, you might also want to throw out numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi There! Can you believe it's ?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove digits using regex\n",
    "digits = r'\\d+'\n",
    "re.sub(digits, '', caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi There! Can you believe it's 2020?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that since we didn't assign the changes to the string variable, the changes aren't \"saved\"\n",
    "caps_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation\n",
    "\n",
    "Sometimes you might want to keep only the alphanumeric characters (the letters and numbers) and ditch the punctuation. Here's how we can do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi There! Can you believe it's 2020\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strip() will remove punctuation from the beginning or end of the string\n",
    "caps_string.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code looks a little complex, but essentially it will move through each character in our ```caps_string``` variable, and replace any punctuation mark from our ```string.punctuation``` list with a blank string, ```''```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi There Can you believe its 2020'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code will return all punctuation from the caps_string variable string\n",
    "''.join(word.strip(string.punctuation) for word in caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove punctuation from our SOTU speech\n",
    "clean_text = ''.join(word.strip(string.punctuation) for word in clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove anything but letters\n",
    "We can use a regular expression that matches only upper and lower case letters to remove everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi There Can you believe it s '"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this case we sub any non-letter characters out with a space, ' '\n",
    "letters_only = r'[^A-Za-z]+'\n",
    "re.sub(letters_only, ' ', caps_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with the Natural Language Toolkit (NLTK)\n",
    "\n",
    "We can also use the Natural Language Toolkit (NLTK) to accomplish many of the steps we showed manually above. Or in the case of our clean_text variable, where we've already removed punctuation, we can use the word_tokenize module to break the text up into its consitutent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'president',\n",
       " 'mr',\n",
       " 'speaker',\n",
       " 'members',\n",
       " 'of',\n",
       " 'the',\n",
       " '104th',\n",
       " 'congress',\n",
       " 'my']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(clean_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tokens we can count their frequencies in the speech. To do that we'll import a 'Counter' tool from the 'collections' package and then apply it to our tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 463),\n",
       " ('to', 401),\n",
       " ('and', 348),\n",
       " ('of', 231),\n",
       " ('we', 203),\n",
       " ('in', 180),\n",
       " ('a', 179),\n",
       " ('our', 140),\n",
       " ('that', 133),\n",
       " ('for', 113)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## counting tokens\n",
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "\n",
    "# Counter includes a most_common method we can apply to our freq variable\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "NLTK includes a stopwords module we can use. Not all stopwords lists are equal though: for your own research you might want to customize a stopwords list, or find one that is best-suited to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# how many stopwords are on the list?\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the first ten word on the stopword list?\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new list of tokens, removing our stopwords along the way. \n",
    "\n",
    "This loop checks each word in our original tokens list, and if it does *not* appear on the stopword list, it adds it to a new list called tokens_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'president',\n",
       " 'mr',\n",
       " 'speaker',\n",
       " 'members',\n",
       " '104th',\n",
       " 'congress',\n",
       " 'fellow',\n",
       " 'americans',\n",
       " 'sanctuary']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_clean = [] \n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop: \n",
    "        tokens_clean.append(w)\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 133),\n",
       " ('it', 95),\n",
       " ('people', 72),\n",
       " ('this', 63),\n",
       " ('work', 42),\n",
       " ('who', 42),\n",
       " ('new', 41),\n",
       " ('us', 37),\n",
       " ('its', 34),\n",
       " ('government', 34)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can re-count the most common words after stop words are removed\n",
    "freq = Counter(tokens_clean)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, still not terribly interesting but getting better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/lemmatization\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'state'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the PorterStemmer and then stem the word \"states\" as an example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unit'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('united')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'govern'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('government')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as the stopwords loop above, we can create a new list of stemmed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed = []\n",
    "for t in tokens_clean:\n",
    "    tokens_stemmed.append(stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'presid',\n",
       " 'mr',\n",
       " 'speaker',\n",
       " 'member',\n",
       " '104th',\n",
       " 'congress',\n",
       " 'fellow',\n",
       " 'american',\n",
       " 'sanctuari']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:\n",
      " [('that', 145), ('it', 129), ('peopl', 72), ('work', 69), ('thi', 63), ('year', 58), ('who', 43), ('new', 41), ('american', 40), ('govern', 39)] \n",
      "\n",
      "Not stemmed:\n",
      " [('that', 133), ('it', 95), ('people', 72), ('this', 63), ('work', 42), ('who', 42), ('new', 41), ('us', 37), ('its', 34), ('government', 34)]\n"
     ]
    }
   ],
   "source": [
    "# now that the words are stemmed, are the most common words any different?\n",
    "freq_stemmed = Counter(tokens_stemmed)\n",
    "print(\"Stemmed:\\n\", freq_stemmed.most_common(10), '\\n')\n",
    "print(\"Not stemmed:\\n\", freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We can read them all into a single variable using a Python tool called glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of the files that end with .txt in the sotu_text/ folder to a variable called sotu_all\n",
    "sotu_all = glob.glob(\"sotu_text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sotu_text/060.txt',\n",
       " 'sotu_text/074.txt',\n",
       " 'sotu_text/048.txt',\n",
       " 'sotu_text/114.txt',\n",
       " 'sotu_text/100.txt',\n",
       " 'sotu_text/128.txt',\n",
       " 'sotu_text/129.txt',\n",
       " 'sotu_text/101.txt',\n",
       " 'sotu_text/115.txt',\n",
       " 'sotu_text/049.txt']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this just saves the file-paths to a list though\n",
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all the files we need to cycle through each one and save the text from the file.\n",
    "\n",
    "To do that we'll create two new list variables, speeches, and files. For each file in the sotu_all variable we'll open and read the file, and save the text to the speeches list. We'll also save the file names to the files list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches = []\n",
    "files = []\n",
    "for speech in sotu_all:\n",
    "    s = open(speech, 'r')\n",
    "    files.append(speech)\n",
    "    text = s.read()\n",
    "    speeches.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. President, Mr. Speaker, Members of the 104th Congress, my fellow Americans: Again we are here in the sanctuary of democracy, and once again our democracy has spoken. So let me begin by congratulating all of you here in the 104th Congress and cong'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can refer to each speech from the list using the list index\n",
    "speeches[45][0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sotu_text/215.txt'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "Some of the code, descriptions, and examples above are taken from:\n",
    "* UC Berkeley's D-Lab [workshop on Text Analysis Fundamentals](https://dlab.berkeley.edu/training/text-analysis-fundamentals-unsupervised-approaches-10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
