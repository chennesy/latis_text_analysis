{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Text Analysis (LATIS/Libraries), Spring 2020\n",
    "\n",
    "## Intro\n",
    "lorem ipsum\n",
    "\n",
    "### Strings\n",
    "To work with text in Python it's important to be able to manipulate string variables. Let's create a variable called text_string and print it as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string = 'Hi there'\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what kind of a variable this is by using the built-in type() function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to specific characters in the string by slicing it using brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also 'add' or *concatenate* strings together using the plus sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string + '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string += '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string + ' ' + 'How are you today?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since we didn't save the concatentations above to a variable the original text_string is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in string methods\n",
    "Strings also have built-in methods that can operate on them. These include *join*, *find*, *replace*, *lower* and *upper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string = 'How are you today, Mike?'\n",
    "how_string.replace('H', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.replace('are you today', 'were you yesterday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'x'.join(how_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "Usually you want to work with text from files though, and not manually create string variables. \n",
    "\n",
    "The first step is to read in the files containing the data. Common file types for text data are: \n",
    "* `.txt`\n",
    "* `.csv`\n",
    "* `.json`\n",
    "* `.html` \n",
    "* `.xml`\n",
    "\n",
    "Each file format requires specific Python tools or methods to read, but for our case, we'll be working with .txt files.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "Let's take a look at the first file in our directory (folder) of State of the Union addresses (`/sotu_text`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new variable called file1 and read (\"r\") the first file in the sotu_text folder\n",
    "file1 = open(\"sotu_text/215.txt\",\"r\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but when we print the variable, it's not yet stored as a string\n",
    "type(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to view the text, let's read in the file1 object to a new variable called \"text\" using .read() and then print out the first250 characters\n",
    "text = file1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Once we've read in the data, a common next step is to split a longer string into words. This step is referred to as \"tokenization\". That's because each occurrence of a word is called a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text.\n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "Let's save each word to a list variable called 'tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the split() function to split the text variable up by whitespace into a tokens list\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of a variable is tokens?\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists \n",
    "You can view each item in a Python list using the same syntax we used above to slice a str variable. The first item in the tokens list is at ```tokens[0]``` and the second is ```tokens[1]```. You can view a range of the first 10 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first one\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last ten\n",
    "tokens[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you can also slice the string variables stored inside of a list\n",
    "tokens[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences, and provides a different way to tokenize our text.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the default whitespace for split(), you can identify the character or characters you'd like to split on\n",
    "sentences = text.split('.')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many items are in any list using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this method doesn't break out sentences that end with other punctuation, like question marks\n",
    "sentences[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "We could improve on this by using regular expressions. They allow us to split strings using specific characters or patterns that match different *kinds* of characters. Regex is a very powerful tool, but we won't go into it much today. For help figuring out and working with regular expressions we recommend https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this pattern matches periods, question marks, or exclamation marks\n",
    "boundary_pattern = r'[.?!]'\n",
    "sentences_re = re.split(boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are now a few more sentences in our list\n",
    "len(sentences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this sentence ends at the question mark\n",
    "sentences_re[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip whitespace\n",
    "\n",
    "This is an extremely common step in text cleaning. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" Hi there! \"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ```strip()``` to remove \"line breaks\" from strings. Line breaks are often represented with \"escape characters\" such as ```\\n``` in text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove whitespace at the beginning and end of a string using .strip()\n",
    "stripped_text = text.strip()\n",
    "stripped_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run more complex find/replace patterns using regex. Here we use ```re.sub()``` to match any \\s+ characters with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use regular expressions to remove whitespace throughout the string\n",
    "# note that we are replacing any of the matching whitespace patterns with a single space ' '.\n",
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization\n",
    "Text normalization can help us clean our text to fit some standard patterns. One common normalization step is to remove case from the text.\n",
    "\n",
    "If you want to count the frequencies of words, for example, using lower case will ensure you don't count \"Death\" and \"death\" as two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_string = \"Hi There! Can you believe it's 2020?\"\n",
    "caps_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text[0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your analysis, you might also want to throw out numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits using regex\n",
    "digits = r'\\d+'\n",
    "re.sub(digits, '', caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that since we didn't assign the changes to the string variable, the changes aren't \"saved\"\n",
    "caps_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation\n",
    "\n",
    "Sometimes you might want to keep only the alphanumeric characters (the letters and numbers) and ditch the punctuation. Here's how we can do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() will remove punctuation from the beginning or end of the string\n",
    "caps_string.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code looks a little complex, but essentially it will move through each character in our ```caps_string``` variable, and replace any punctuation mark from our ```string.punctuation``` list with a blank string, ```''```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will return all punctuation from the caps_string variable string\n",
    "''.join(word.strip(string.punctuation) for word in caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's remove punctuation from our SOTU speech\n",
    "clean_text = ''.join(word.strip(string.punctuation) for word in clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what is called a *comprehension* in Python. A way to iterate or loop over multiple similar items, perform a task, and capture the result of that task for each of the items in a single object. It is very consise, and a powerful way to think about repetitive tasks, like text cleaning.\n",
    "\n",
    "They're easiest to understand by going backwards from the loop and conditions and then seeing what is done to them.\n",
    "\n",
    "So first, we're looping over each item (which we're calling *word* in the list *clean_text*:\n",
    "Then, we're running the .strip() method on that item and stripping all punctuation as listed in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as a for-loop, this would look like the following, but the output wouldn't be saved\n",
    "for word in clean_text:\n",
    "    word.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a list comprehension, we can capture the output into a single list\n",
    "output = [word.strip(string.punctuation) for word in clean_text]\n",
    "output[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want that result as a single string, rather than a list of separate words, so we're using some Python slight of hand to make that happen: we're joining, or concatenating each item of that list to an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also overwriting the clean_text with the puctuation-stripped string, which is why you see that same variable on both the left and the right hand of the equals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_text = ''.join(word.strip(string.punctuation) for word in clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove anything but letters\n",
    "We can use a regular expression that matches only upper and lower case letters to remove everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case we sub any non-letter characters out with a space, ' '\n",
    "letters_only = r'[^A-Za-z]+'\n",
    "re.sub(letters_only, ' ', caps_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with the Natural Language Toolkit (NLTK)\n",
    "\n",
    "We can also use the Natural Language Toolkit (NLTK) to accomplish many of the steps we showed manually above. Or in the case of our clean_text variable, where we've already removed punctuation, we can use the word_tokenize module to break the text up into its consitutent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(clean_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tokens we can count their frequencies in the speech. Let's use a builtin NLTK function called FreqDist() to look at our most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the FreqDist function to our tokens variable\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "#fdist is a dictionary of unique words and the number of times they occur\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "A Python dictionary is a way to hold an unordered list of items, using something called a 'key:value' pair. Above you can see the list of keys (tokens) and values (word counts) from the FreqDist dictionary. A good way to differentiate a Python dictionary from a Python list is to look at the brackets used:\n",
    "\n",
    "* ```{}``` curly brackets for dictionaries\n",
    "* ```[]``` square brackets for lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it also includes a handy method to find the most common words \n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "NLTK includes a stopwords module we can use. Not all stopwords lists are equal though: for your own research you might want to customize a stopwords list, or find one that is best-suited to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# how many stopwords are on the list?\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the first ten word on the stopword list?\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new list of tokens, removing our stopwords along the way. \n",
    "\n",
    "This loop checks each word in our original tokens list, and if it does *not* appear on the stopword list, it adds it to a new list called tokens_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean = [] \n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop: \n",
    "        tokens_clean.append(w)\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# advanced we can do the same thing quite efficiently with a list comprehension\n",
    "tokens_clean = [w for w in tokens if w not in stop]\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can re-count the most common words after stop words are removed\n",
    "freq = FreqDist(tokens_clean)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, still not terribly interesting but getting better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PorterStemmer and then stem the word \"states\" as an example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('united')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('government')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as the stopwords loop above, we can create a new list of stemmed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_stemmed = []\n",
    "for t in tokens_clean:\n",
    "    tokens_stemmed.append(stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or as a comprehension:\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are stemmed, are the most common words any different? \n",
    "\n",
    "Here are the stemmed top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stemmed = FreqDist(tokens_stemmed)\n",
    "for f in freq_stemmed.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unstemmed top ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in freq.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar, but with some important differences. Notice that \"work\" went from 42 to 69 after stemming.  \n",
    "\n",
    "Why would that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We can read them all into a single variable using a Python tool called glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all of the files that end with .txt in the sotu_text/ folder to a variable called sotu_all\n",
    "sotu_all = glob.glob(\"sotu_text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just saves the file-paths to a list though\n",
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are out of order though. Let's sort the list so that the list index is in the same order as the speeches themselves (sotu_all[0] would equal 001.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all the files we need to cycle through each one and save the text from the file.\n",
    "\n",
    "To do that we'll create a new list variable, speeches. For each file in the sotu_all variable we'll open and read the file, and save the text to the speeches list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for speech in sotu_all:\n",
    "    s = open(speech, 'r')\n",
    "    text = s.read()\n",
    "    speeches.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can refer to each speech from the list using the list index\n",
    "speeches[45][0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which file is that?\n",
    "sotu_all[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short function to tidy the open/append loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = [open(speech, 'r').read() for speech in sotu_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[235][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Functions\n",
    "Now that we have all the text data loaded, we can think about working on the corpus as a whole.\n",
    "\n",
    "Let's create a function that combines all of our cleaning protocols so that we can clean each State of the Union speech with a single piece of code. \n",
    "\n",
    "The function definition opens with the keyword ```def``` followed by the name of the function (clean_speech) and a parenthesized list of parameter names (speech). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speech(speech):\n",
    "    speech = ''.join(word.strip(string.punctuation) for word in speech.lower())\n",
    "    speech = [stemmer.stem(w) for w in word_tokenize(speech) if w not in stop]\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the function using the name of the function, and the variable you'd like to process as its parameter. To process only the first speech, for example, you could call:\n",
    "\n",
    "```clean_speech(speeches[0])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also assign the function's output to a variable so you can work with it later:\n",
    "\n",
    "```first_cleaned = clean_speech(speeches[0])```\n",
    "\n",
    "Let's put it all together and clean all of the speeches, and assign them to a new list, ```cleaned_speeches```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_speeches = [clean_speech(speech) for speech in speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each item in the cleaned_speeches list is also a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cleaned_speeches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies (reprise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note to Mike: \n",
    "I played around with ConditionalFreqDist but it quickly got a little dense for my taste, in terms of teaching. Tough to unpack the \"what's going on here\" aspect. Might just be me though. Happy to re-focus here if you think we should. \n",
    "\n",
    "Instead I go back to the FreqDist tool and sort of manually build some of these visualizations across the corpus. I like how that reinforces some basic lessons on lists, dictionaries and loops. The downside is that it requires introducing pandas and matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk import ConditionalFreqDist\n",
    "\n",
    "##need to fix this\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target)) [1]\n",
    "cfd.plot()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know we can get word frequencies from a speech. Let's apply that to the entire cleaned and stemmed corpus we've created.\n",
    "\n",
    "First, remember how we can look at the frequencies for a single document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(cleaned_speeches[100])\n",
    "fd.most_common()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "We can use a tool called matplotlib to help visualize some of our results. NLTK uses matplotlib as the engine for their .plot() function, but let's install it here and also run some Jupyter \"magic\" to make sure the matplotlib visualizations appear inline (in the display) of our Jupyer view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the built-in plot() function to visualize the FreqDist for our first ten words. Unfortunately the .plot() tool here only work with line graphs. A bar chart would be better! We can do that later with the full matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the built-in plot function for FreqDist \n",
    "fd.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enumerate() & building a Python dictionary\n",
    "\n",
    "The following short loop uses a couple of essential Python tools. \n",
    "\n",
    "1. Let's create our own Python dictionary, which we'll call freq_dicts, to keep track of each speech. Each dictionary entry will represent a speech from the corpus. The dictionary \"key\" will be the index for the speech, and the \"value\" will be the FreqDist data for that speech. Remember that the FreqDist data is also stored as a dictionary. So the value of each speech is itself a dictionary.\n",
    "\n",
    "2. Since we want to reference the index for each speech in the loop, we can use the Python enumerate() function. Enumerate allows us to basically keep a count in a for loop, and to reference the enumerate variable. We'll call our enumerate variable 'idx' for index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dists = {}\n",
    "for idx, speech in enumerate(cleaned_speeches):\n",
    "    freq_dists[idx] = FreqDist(speech) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the last speech in the freq_dists dictionary by referencing its key in the same way we would reference a list index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dists[235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then hone in on a specific key from the dictionary that is inside of the value of speech 235 (confusing, I know!), by again calling the key name. Note that in this case the key is a string (american) so we need to put it in single quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dists[235]['american']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new for loop to look at the occurrence of the term american in all of our speeches. We can use a very similar technique as above to capture the data in a new dictionary. \n",
    "\n",
    "* ```americans[speech[0]]``` assigns the key from freq_dists (which is the index number) to our new americans dictionary\n",
    "* ```speech[1]['american']``` assigns the value of the dictionary within the dictionary (FreqDist) that matches the 'american' key (which is the count of the number of times 'american' appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "americans = {}\n",
    "for speech in freq_dists.items():\n",
    "    americans[speech[0]] =  speech[1]['american']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#americans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas & dataframes\n",
    "Pandas is a flexible data analysis tool for Python. R users will be familiar with data frames (and probably be a little frustrated with how Pandas implements dataframes, because they're a little different!). We're going to use Pandas in a very simple way, to take a closer look at our data in a tabular form, and then to visualize using matplotlib.\n",
    "\n",
    "The following defines a new variable df_americans, in which we instantiate the .DataFrame.from_dict() tool. We tell it to create the dataframe from the americans dictionary, we set the orientation so that the dictionary keys are the index for the dataframe, then we assign the column name \"count\" for the word count column (which is from the dictionary values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dictionary to pandas\n",
    "import pandas as pd\n",
    "df_americans = pd.DataFrame.from_dict(americans, orient='index')\n",
    "df_americans.columns = ['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily peak at the first 5 rows of a pandas dataframe using .head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_americans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can sort the data by any column using sort_values(). Let's see who says 'american' the most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_americans.sort_values(by='count', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our index matches our ```speeches``` index we can take a closer look at speech 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who says \"americans\" the most?\n",
    "speeches[123][-250:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use matplotlib to chart the occurrence of the word 'american' over time in the full corpus.\n",
    "\n",
    "* The x-axis is the index for the df_americans dataframe (each speech index)\n",
    "* The y-axis is the word count number from the dataframe.\n",
    "* Then we can just add some labels, a title, and call the matplotlib function ```.show()``` to take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = df_americans.index\n",
    "y_count = df_americans['count']\n",
    "plt.plot(x_speech, y_count)\n",
    "plt.ylabel(\"Count of \\'Americans\\'\")\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.title(\"Use of \\'Americans\\' over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge questions:\n",
    "* What's suboptimal about the chart above? \n",
    "* How could we clean up this data to create a more meaningful chart? \n",
    "* What's missing in the data we're using here? \n",
    "* How could we visualize it better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is an exploratory data analysis technique that \"seeks to quantify the emotional intensity of words and phrases within a text.\" (quote from the [Programming Historian SA tutorial](https://programminghistorian.org/en/lessons/sentiment-analysis))\n",
    "\n",
    "We can use more NLTK tools to run a simple sentiment analysis on our SOTU corpus. To download files from NLTK we first need to import the full package. We'll also want some sentiment-specific modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the toolbox we need, let's create a sentiment tokenizer using the English language sentiment library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can initialize the vader SentimentIntensityAnalyzer and save it to a variable called sid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the features of the sentiment analysis tool. You can take a look at some of those features by typing sid. and then tabbing through the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the 'polarity_scores' for a specific speech. For Sentiment analysis we don't need the cleaned speech, so we'll go back to our original speeches list.\n",
    "\n",
    "polarity_scores will give us positive and negative scores. This feature is built into VADER and can be requested on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sid.polarity_scores(speeches[100])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "We can format those by looping through the scores dictionary. Remember that dictionaries are key:value pairs stored in curly brackets. We can cycle through the scores dictionary like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(scores):\n",
    "    print('{0}: {1}'.format(key, scores[key]), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the scores for the entire speeches corpus.\n",
    "We'll create another dictionary, 'all_scores', that will use the speeches index as the key, and the scores as its value. Note that this means that the value for each item in 'all_scores' will contain *another* dictionary.\n",
    "\n",
    "This might take a few minutes to run because it has to analyze all 235 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for idx, speech in enumerate(speeches):\n",
    "    all_scores[idx] = sid.polarity_scores(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the scores for specific speeches by referencing the index/key of all_scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific score by referencing the key within the scores dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can list all of the negative scores for the corpus. \n",
    "\n",
    "To keep it somewhat simple, let's just create a new dictionary that will only contain negative scores. We can create an empty dictionary called negative, then cycle through each key:value item in the all_scores dictionary from above. For each item, we'll assign the index number as its key and the negative score as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = {}\n",
    "for score in all_scores.items():\n",
    "    negative[score[0]] =  score[1]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a Python tool called Pandas to look at the negative dictionary in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(negative, orient='index')\n",
    "df.columns = ['neg_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the dataframe using a builtin method of Pandas called sort_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Matplotlib again here to plot out the negative scores for the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = df.index\n",
    "y_neg = df['neg_value']\n",
    "plt.plot(x_speech, y_neg)\n",
    "plt.ylabel(\"Negative value\")\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.title('Negativity of speeches over time')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negative speeches\n",
    "The graph gives us a nice visualization of some overall trends, but it's hard to identify specific speeches here. We can just sort our dataframe to look at the most negatively scored speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='neg_value', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[222][0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least negative speeches\n",
    "We can use the default sort (ascending values) to view the least negative speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='neg_value')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "In topic models each document is represented as a distribution over topics, and each topic is represented as a distribution over words. \n",
    "\n",
    "Topic model algorithms require documents to be transformed into a document-term-matrix (DTM). In a DTM each row represents a document (for us, a SOTU speech), each column represents a specific word or token in the corpus, and the matrix contains the number of times each word appears in a given document. A document can be a tweet, a novel, or an entire corpus of an author's work. How you define the boundaries of a document makes a huge difference to the output of a topic model algorithm.\n",
    "\n",
    "Let's consider two documents, each one sentence long: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"The brown fox jumped over the white cow.\"\n",
    "sentence_2 = \"The red hen ran away from the brown fox.\"\n",
    "tokens_s1 = clean_speech(sentence_1)\n",
    "tokens_s2 = clean_speech(sentence_2)\n",
    "print(tokens_s1)\n",
    "print(tokens_s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we would represent those two documents in a document-term-matrix, after stemming and throwing out stop words:\n",
    "\n",
    "|brown | fox | jump | white | cow | red | hen | ran | away|\n",
    "|--|--| --| --| --| --| --| --| --|\n",
    "|1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0|\n",
    "|1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED DAVID TO INSTALL GENSIM IN THE HUB\n",
    "\n",
    "### Gensim\n",
    "\n",
    "Gensim is a popular Python library built specifically for topic modeling. While other popular data science libraries, such as scikit-learn, can be used for topic modeling, gensim has a lot of handy built-in features that will help us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import sys\n",
    "!{sys.executable} -m pip install gensim'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply gensim to our cleaned_speeches corpus. \n",
    "\n",
    "First, we want to run the ```corpora.Dictionary()``` function from Gensim to create a dictionary of all of the terms in the corpus. Within the dictionary object is a long list of every unique word/token. In our case there are 23,662."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(cleaned_speeches)\n",
    "print(\"dictionary:\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also available within the gensim dictionary object, is a Python dictionary, ```.token2id```, that contains the unique terms from the tokens list as keys, and the values are an index for each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to convert the lists of tokens from cleaned_speeches to a bag-of-words model using the ```doc2bow()``` function. This bit of code uses list comprehension to cycle through every tokenized speech (here called 'text') in cleaned_speeches and apply the doc2.bow() function to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in cleaned_speeches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus doesn't look like much initially. Here are the first 20 items in a list for the first item/speech in the entire corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function doc2bow() counts the number of occurrences of each distinct word, converts the word to its integer word id and returns the result as a sparse vector. So the above is a list of the first 20 token ids + counts from the first speech, corpus[0].\n",
    "\n",
    "Say what?! Well, let's take a look at a simple example using a sentence from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec = dictionary.doc2bow(tokens_s1)\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using the dictionary object for the cleaned_speeches corpus still, the first number in each pair above — ```(2114,1)``` - refers to the id for that token in the dictionary from cleaned_speeches. The second number refers to the number of times it occurs in our tokens list *tokens_s1*.\n",
    "\n",
    "If we want to save our corpus and dictionary objects to work with later, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA topic models\n",
    "Now let's use gensim to create an LDA (Latent Dirichlet Allocation) topic model with our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models require us to set the number of topics ahead of time. Let's guess there are about 20 topics in these speeches.\n",
    "\n",
    "LDA on a large-ish corpus like ours will take a little while to run since it uses machine learning to iterate over the corpus in many different passes, adjusting its findings as it \"learns\" what topics \"fit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 20\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the model, but LDA doesn't actually tell us what topics are called. It doesn't actually understand a topic at all, that's our job. What it can tell us is which words are most strongly associated with each topic. From there we might see clear patterns and label the topics as we like. \n",
    "\n",
    "Here we use the print_topics() function, which takes an argument for how many words per topic we want to see, and cycle through each topic to view the associated words, along with their \"prevalence\" score to the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words = 10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, those topics are really repetitive and kind of meaningless! How can we improve these?\n",
    "\n",
    "Seems like we want to remove more words from our corpus since the most common words (state, nation, govern...) show up over and over here. We could go back and manually add those stop words to our stop word list, but we have an easier option: we modify our gensim dictionary with a filter to remove the *n* most frequent terms. \n",
    "\n",
    "Let's try to remove the most common 25 words and re-run our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_n_most_frequent(25)\n",
    "\n",
    "# since we've updated the dictionary we have to re-assign the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_speeches]\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model_20.gensim')\n",
    "#https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we re-run the topic word list code — adding an enumerate() counter so we can print out each topic number a little more clearly - the topics become more interesting and some are easy to label. \n",
    "\n",
    "Some of these are still weird though! Topic models can be a nice way to see what parts of the data still need to be cleaned. Looking at this output, what else do you think we could do to clean up our corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_words = 10)\n",
    "for i, topic in enumerate(topics):\n",
    "    print(\"Topic\", i, \"\\n\", topic, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific speech, to see what topics are strongly associated with it, using the dictionary .doc2bow() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bush_bow = dictionary.doc2bow(cleaned_speeches[235])\n",
    "print(ldamodel.get_document_topics(bush_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kennedy_bow = dictionary.doc2bow(cleaned_speeches[177])\n",
    "print(ldamodel.get_document_topics(kennedy_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "washington_bow = dictionary.doc2bow(cleaned_speeches[0])\n",
    "print(ldamodel.get_document_topics(washington_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing topic models\n",
    "\n",
    "There's one more cool tool we can use with gensim - pyLDAvis - to actually visualize what these topics look like, what words are associated with them, and how distant they are from each other in a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model20.gensim')\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you create a great visualization using pyLDAvis you can save it as an html file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(lda_display,'lda_viz.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - can you change the model to have fewer, more meaningful topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - can you remove less frequent terms that might be creating unwanted noise in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "Some of the code, descriptions, and examples above are taken from:\n",
    "* UC Berkeley's D-Lab [workshop on Text Analysis Fundamentals](https://dlab.berkeley.edu/training/text-analysis-fundamentals-unsupervised-approaches-10).\n",
    "* Software Carpentry's open source [Python curriculum](http://swcarpentry.github.io/python-novice-inflammation/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
