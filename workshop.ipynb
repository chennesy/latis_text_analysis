{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Text Analysis (UMN LATIS/Libraries)\n",
    "\n",
    "## Intro\n",
    "During this session, we'll go from the basics of working with text (strings) in Python, to plotting sentiment and topics of a corpus of texts. \n",
    "- String Manipulation & Methods\n",
    "- Reading in Text Files\n",
    "- Text Cleaning (Tokenization & Stemming)\n",
    "- Word Frequency\n",
    "- Sentiment\n",
    "\n",
    "We'll also cover basics of working with Python, including data structures like lists and dictionaries, creating functions, list comprehensions, and importing methods and classes from different libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics\n",
    "- Zoom Layout \n",
    "    - Turn off Side By Side in view options\n",
    "    - Set view to 100%\n",
    "    - Multiple monitors will help\n",
    "    - Please use the Chat window for questions; we'll be monitoring\n",
    "- Overview of JupyterLabs\n",
    "- Scaling Level of Content\n",
    "    - Our first hour will be entry-level \n",
    "    - By the end, more advanced\n",
    "    - So hopefully something for everyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings\n",
    "To work with text in Python it's important to be able to manipulate string variables. Let's create a variable called text_string and print it as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string = 'Hi there'\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what kind of a variable this is by using the built-in type() function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refer to specific characters in the string by slicing it using brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_string[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_string[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also 'add' or *concatenate* strings together using the plus sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string + '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string += '!'\n",
    "print(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_string + ' ' + 'How are you today?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_var = \"here is my string: I want to find the colon\"\n",
    "count_colons = 0\n",
    "for t in text_var:\n",
    "    if t == ':':\n",
    "        count_colons +=1\n",
    "print(count_colons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in string methods\n",
    "Strings also have built-in methods that can operate on them. These include *join*, *find*, *replace*, *lower* and *upper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string = 'How are you today, Mike?'\n",
    "how_string.replace('H', 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.replace('are you today', 'were you yesterday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'x'.join(how_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading files\n",
    "\n",
    "Usually you want to work with text from files though, and not manually create string variables. \n",
    "\n",
    "The first step is to read in the files containing the data. Common file types for text data are: \n",
    "* `.txt`\n",
    "* `.csv`\n",
    "* `.json`\n",
    "* `.html` \n",
    "* `.xml`\n",
    "\n",
    "Each file format requires specific Python tools or methods to read, but for our case, we'll be working with .txt files.\n",
    "\n",
    "#### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files.\n",
    "\n",
    "Let's take a look at the first file in our directory (folder) of State of the Union addresses (`/sotu_text`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new variable called file1 and read (\"r\") the first file in the sotu_text folder\n",
    "file1 = open(\"sotu_text/215.txt\",\"r\") \n",
    "print(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but when we print the variable, it's not yet stored as a string\n",
    "type(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to view the text, let's read in the file1 object to a new variable called \"text\" using .read() and then print out the first250 characters\n",
    "text = file1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Once we've read in the data, a common next step is to split a longer string into words. This step is referred to as \"tokenization\". That's because each occurrence of a word is called a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text.\n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "Let's save each word to a list variable called 'tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the split() function to split the text variable up by whitespace into a tokens list\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for t in tokens:\n",
    "    if t == 'President':\n",
    "        counter += 1\n",
    "print(counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what kind of a variable is tokens?\n",
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists \n",
    "You can view each item in a Python list using the same syntax we used above to slice a str variable. The first item in the tokens list is at ```tokens[0]``` and the second is ```tokens[1]```. You can view a range of the first 10 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first one\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last ten\n",
    "tokens[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you can also slice the string variables stored inside of a list\n",
    "tokens[1][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences, and provides a different way to tokenize our text.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of the default whitespace for split(), you can identify the character or characters you'd like to split on\n",
    "sentences = text.split('.')\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how many items are in any list using the len() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this method doesn't break out sentences that end with other punctuation, like question marks\n",
    "sentences[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions*\n",
    "We could improve on this by using regular expressions. They allow us to split strings using specific characters or patterns that match different *kinds* of characters. Regex is a very powerful tool, but we won't go into it much today. For help figuring out and working with regular expressions we recommend https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll still use a fairly naive segmentation, breaking on any period, question mark, or exclamation point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this pattern matches periods, question marks, or exclamation marks\n",
    "boundary_pattern = r'[.?!]'\n",
    "sentences_re = re.split(boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression above is bounded in *r' '* to let Python know it is a regular expression.\n",
    "\n",
    "Within that the square brackets *[  ]* specify a group of characters that you want to select from. Here you want to match any . ? or !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are now a few more sentences in our list\n",
    "len(sentences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this sentence ends at the question mark\n",
    "sentences_re[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip whitespace\n",
    "\n",
    "This is an extremely common step in text cleaning. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \" Hi there! \"\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ```strip()``` to remove \"line breaks\" from strings. Line breaks are often represented with \"escape characters\" such as ```\\n``` in text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can remove whitespace at the beginning and end of a string using .strip()\n",
    "stripped_text = text.strip()\n",
    "stripped_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run more complex find/replace patterns using regex. Here we use ```re.sub()``` to match any \\s+ characters with a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use regular expressions to remove whitespace throughout the string\n",
    "# note that we are replacing any of the matching whitespace patterns with a single space ' '.\n",
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization\n",
    "Text normalization can help us clean our text to fit some standard patterns. One common normalization step is to remove case from the text.\n",
    "\n",
    "If you want to count the frequencies of words, for example, using lower case will ensure you don't count \"Death\" and \"death\" as two separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_string = \"Hi There! Can you believe it's 2021?\"\n",
    "caps_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = clean_text.lower()\n",
    "clean_text[0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your analysis, you might also want to throw out numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove digits using regex\n",
    "digits = r'\\d+'\n",
    "re.sub(digits, '', caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that since we didn't assign the changes to the string variable, the changes aren't \"saved\"\n",
    "caps_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation\n",
    "\n",
    "Sometimes you might want to keep only the alphanumeric characters (the letters and numbers) and ditch the punctuation. Here's how we can do that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip() will remove punctuation from the beginning or end of the string\n",
    "caps_string.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code looks a little complex, but essentially it will move through each character in our ```caps_string``` variable, and replace any punctuation mark from our ```string.punctuation``` list with a blank string, ```''```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will return all punctuation from the caps_string variable string\n",
    "''.join(word.strip(string.punctuation) for word in caps_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's remove punctuation from our SOTU speech\n",
    "clean_text = ''.join(word.strip(string.punctuation) for word in clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what is called a *comprehension* in Python. A way to iterate or loop over multiple similar items, perform a task, and capture the result of that task for each of the items in a single object. It is very consise, and a powerful way to think about repetitive tasks, like text cleaning.\n",
    "\n",
    "They're easiest to understand by going backwards from the loop and conditions and then seeing what is done to them.\n",
    "\n",
    "So first, we're looping over each item (which we're calling *char* in the list *clean_text*:\n",
    "Then, we're running the .strip() method on that item and stripping all punctuation as listed in string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as a for-loop, this would look like the following, but the output wouldn't be saved\n",
    "for char in clean_text:\n",
    "    char.strip(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as a list comprehension, we can capture the output into a single list\n",
    "output = [char.strip(string.punctuation) for char in clean_text]\n",
    "output[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want that result as a single string, rather than a list of separate words, so we're using some Python slight of hand to make that happen: we're joining, or concatenating each item of that list to an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also overwriting the clean_text with the puctuation-stripped string, which is why you see that same variable on both the left and the right hand of the equals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_text = ''.join(char.strip(string.punctuation) for char in clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove anything but letters\n",
    "We can use a regular expression that matches only upper and lower case letters to remove everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case we sub any non-letter characters out with a space, ' '\n",
    "print('original string:\\n', caps_string)\n",
    "letters_only = r'[^A-Za-z]+'\n",
    "re.sub(letters_only, ' ', caps_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to step through the RegEx:\n",
    "\n",
    "- *r' '* bounds the string as a regular expression.\n",
    "- The square brackets *[  ]* create a group, or class of characters you want to match. In this case: all capital letters A-Z OR all lowercase letters a-z\n",
    "- The caret at the start of this group *^* negates that grouping.\n",
    "\n",
    "So, as a result, we match any time we find a non-letter, and the replace it with a space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with the Natural Language Toolkit (NLTK)* *\n",
    "\n",
    "We can also use the Natural Language Toolkit (NLTK) to accomplish many of the steps we showed manually above. Or in the case of our clean_text variable, where we've already removed punctuation, we can use the word_tokenize module to break the text up into its consitutent tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(clean_text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of tokens we can count their frequencies in the speech. Let's use a builtin NLTK function called FreqDist() to look at our most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the FreqDist function to our tokens variable\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "#fdist is a dictionary of unique words and the number of times they occur\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "A Python dictionary is a way to hold an unordered list of items, using something called a 'key:value' pair. Above you can see the list of keys (tokens) and values (word counts) from the FreqDist dictionary. A good way to differentiate a Python dictionary from a Python list is to look at the brackets used:\n",
    "\n",
    "* ```{}``` curly brackets for dictionaries\n",
    "* ```[]``` square brackets for lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it also includes a handy method to find the most common words \n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which, to make things even more complicated, returns a list (see the square brackets continaing comma-separated items) containing tuples (those objects in parentheses, also containing comma-separated items). But we needn't get overly worried about that here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "NLTK includes a stopwords module we can use. Not all stopwords lists are equal though: for your own research you might want to customize a stopwords list, or find one that is best-suited to your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# how many stopwords are on the list?\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the first ten word on the stopword list?\n",
    "stop[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new list of tokens, removing our stopwords along the way. \n",
    "\n",
    "This loop checks each word in our original tokens list, and if it does *not* appear on the stopword list, it adds it to a new list called tokens_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_clean = [] \n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop: \n",
    "        tokens_clean.append(w)\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# advanced we can do the same thing quite efficiently with a list comprehension\n",
    "tokens_clean = [w for w in tokens if w not in stop]\n",
    "tokens_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can re-count the most common words after stop words are removed\n",
    "freq = FreqDist(tokens_clean)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, still not terribly interesting but getting better..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PorterStemmer and then stem the word \"states\" as an example\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('roosevelt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner as the stopwords loop above, we can create a new list of stemmed tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_stemmed = []\n",
    "for t in tokens_clean:\n",
    "    tokens_stemmed.append(stemmer.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or as a comprehension:\n",
    "tokens_stemmed = [stemmer.stem(t) for t in tokens_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are stemmed, are the most common words any different? \n",
    "\n",
    "Here are the stemmed top ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stemmed = FreqDist(tokens_stemmed)\n",
    "for f in freq_stemmed.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the unstemmed top ten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in freq.most_common(10):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar, but with some important differences. Notice that \"work\" went from 42 to 69 after stemming.  \n",
    "\n",
    "Why would that be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We can read them all into a single variable using a Python tool called glob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save all of the files that end with .txt in the sotu_text/ folder to a variable called sotu_all\n",
    "sotu_all = glob.glob(\"sotu_text/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just saves the file-paths to a list though\n",
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are out of order though. Let's sort the list so that the list index is in the same order as the speeches themselves (sotu_all[0] would equal 001.txt).\n",
    "\n",
    "*Something important to note is that **glob** can pull files differently on different systems (Windows/Mac OS/Linux). If you have a numeric identifier to your files, sorting them is always a good idea for reproducibility of your code, regardless of what system it may be run on*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_all[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all the files we need to cycle through each one and save the text from the file.\n",
    "\n",
    "To do that we'll create a new list variable, speeches. For each file in the sotu_all variable we'll open and read the file, and save the text to the speeches list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = []\n",
    "for speech in sotu_all:\n",
    "    s = open(speech, 'r')\n",
    "    text = s.read()\n",
    "    speeches.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can refer to each speech from the list using the list index\n",
    "speeches[45][0:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which file is that?\n",
    "sotu_all[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a short comprehension version to tidy the open/append loop found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speeches = [open(speech, 'r').read() for speech in sotu_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[235][0:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Cleaning Function*\n",
    "Now that we have all the text data loaded, we can think about working on the corpus as a whole.\n",
    "\n",
    "Let's create a function that combines all of our cleaning protocols so that we can clean each State of the Union speech with a single piece of code. \n",
    "\n",
    "The function definition opens with the keyword ```def``` followed by the name of the function (clean_speech) and a parenthesized list of parameter names (speech). The body of the function — the statements that are executed when it runs — is indented below the definition line. The body concludes with a return keyword followed by the return value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speech(speech):\n",
    "    speech = ''.join(word.strip(string.punctuation) for word in speech.lower())\n",
    "    speech = [stemmer.stem(w) for w in word_tokenize(speech) if w not in stop]\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the function using the name of the function, and the variable you'd like to process as its parameter. To process only the first speech, for example, you could call:\n",
    "\n",
    "```clean_speech(speeches[0])```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also assign the function's output to a variable so you can work with it later:\n",
    "\n",
    "```first_cleaned = clean_speech(speeches[0])```\n",
    "\n",
    "Let's put it all together and clean all of the speeches, and assign them to a new list, ```cleaned_speeches```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell might take a few minutes to run!\n",
    "cleaned_speeches = [clean_speech(speech) for speech in speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_speeches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each item in the cleaned_speeches list is also a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cleaned_speeches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_speeches[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies with ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "cfd = nltk.ConditionalFreqDist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "We can use a tool called matplotlib to help visualize some of our results. NLTK uses matplotlib as the engine for their .plot() function, but let's call it explicitly here (so we can use it separately later) and also run some Jupyter \"magic\" to make sure the matplotlib visualizations appear inline (in the display) of our Jupyter view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enumerate()\n",
    "* The built-in enumerate() function allows us to keep a count of our place in a for loop, and to reference the enumerated variable counter later on. We'll call our counter variable 'i' for index. \n",
    "* The first for-loop iterates through each speech in cleaned_speeches\n",
    "* The second for-loop iterates through each word in the speech at hand.\n",
    "* We'll unpack the ```cfd['americ'][i]+=1``` code a bit more later on, but note for now that ```+=1``` counts each occurence of any word that starts with 'americ' and ```[i]``` refers to the speech index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,speech in enumerate(cleaned_speeches):\n",
    "    for w in speech:\n",
    "        if w.startswith('americ'):\n",
    "            cfd['americ'][i]+=1            \n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the cfd object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a specialized class from nltk, but we interact with it as a Python dictionary. So we can request the items() in cfd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the dict_item key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for cfd dict_key ['americ'] is a FreqDist object. Within FreqDist() there's a regular Python dictionary. In that dictionary, the keys are the speech index, and the values are the term count we supplied in the nested loops above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['americ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have supplied additional 'conditionals' or terms for the distribution to create a comparative line chart, as well. That would have added additional items to the cfd dictionary.\n",
    "\n",
    "Here, we can hone in on a specific key from the dictionary that is inside FreqDist, by again calling the key name - which we assigned above to be the speech index. Inside the CFD loop ```cfd['americ'][i]+=1``` the ```[i]``` assigned each key to match the enumerate value, which was the speech index. So the following should show us the number of times words starting with 'americ' appear in the first speech in our corpus:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['americ'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the 200th speech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['americ'][199]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to see from the plot above which specific speeches used the term the most. We can sort the cfd['americ'] dictionary by values to view the highest counts for 'americ'.\n",
    "\n",
    "We can easily sort sets of values in Python using the sorted() method. Since our counts are the 'value' portion of the 'cfd['americ']' dictionary, with the index as they 'key', we'll want to specify we want to sort the values specifically. By default, Python sorts ascending, so we'll need to specify we want to 'reverse' the sort. Let's see who uses terms like 'america' and 'americans' the most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cfd['americ'].values(), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that above we just get the frequencies of 'americ' terms appearing, rather than the index/key or identifier to know which speech that count occurs in. We can specify we want the identifier as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cfd['americ'], key=cfd['americ'].get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know the speech at the index [123] has the highest count of 'americ' terms. Let's take a closer look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who says \"americ...\" the most?\n",
    "speeches[123][-250:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed plotting\n",
    "If we want more control over the plot, we can use MatPlotLib to set some parameters more specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = list(cfd['americ'].keys())\n",
    "y_count = list(cfd['americ'].values())\n",
    "fig=plt.figure(figsize=(12, 8), dpi= 80)\n",
    "plt.plot(x_speech, y_count)\n",
    "plt.ylabel(\"Count of \\'americ*\\'\")\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.title(\"Use of \\'americ*\\' over time\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis*\n",
    "\n",
    "Sentiment analysis is an exploratory data analysis technique that \"seeks to quantify the emotional intensity of words and phrases within a text.\" (quote from the [Programming Historian SA tutorial](https://programminghistorian.org/en/lessons/sentiment-analysis))\n",
    "\n",
    "We can use more NLTK tools to run a simple sentiment analysis on our SOTU corpus. We'll download the vader_lexicon for sentiment analysis and the Vader and Sentiment modules. Don't worry if you see a warning that we don't have the twython library. We won't be using that since we're not analyzing twitter text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could use a tokenizer that works best for sentiment analysis (see the commented out code below). Since we've already tokenized our text we'll stick with that corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the vader SentimentIntensityAnalyzer and save it to a variable called sid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the features of the sentiment analysis tool. You can take a look at some of those features by typing sid. and then tabbing through the options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's look at the 'polarity_scores' for a specific speech. For Sentiment analysis we don't need the cleaned speech, so we'll go back to our original speeches list.\n",
    "\n",
    "polarity_scores will give us positive and negative scores. This feature is built into VADER and can be requested on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sid.polarity_scores(speeches[100])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "We can format the output by looping through the scores dictionary. Remember that dictionaries are key:value pairs stored in curly brackets. We can cycle through the scores dictionary like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(scores):\n",
    "    print('{0}: {1}'.format(key, scores[key]), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the scores for the entire speeches corpus.\n",
    "We'll create another dictionary, 'all_scores', that will use the speeches index as the key, and the scores as its value. Note that this means that the value for each item in 'all_scores' will contain *another* dictionary.\n",
    "\n",
    "This might take a few minutes to run because it has to analyze all 235 speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for idx, speech in enumerate(speeches):\n",
    "    all_scores[idx] = sid.polarity_scores(speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the scores for specific speeches by referencing the index/key of all_scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a specific score by referencing the key within the scores dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores[235]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can list all of the negative scores for the corpus. \n",
    "\n",
    "To keep it somewhat simple, let's just create a new dictionary that will only contain negative scores. We can create an empty dictionary called negative, then cycle through each key:value item in the all_scores dictionary from above. For each item, we'll assign the index number as its key and the negative score as its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = {}\n",
    "for score in all_scores.items():\n",
    "    negative[score[0]] =  score[1]['neg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Matplotlib again here to plot out the negative scores for the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_speech = list(negative.keys())\n",
    "y_neg = list(negative.values())\n",
    "fig=plt.figure(figsize=(10, 8), dpi= 80)\n",
    "plt.plot(x_speech, y_neg)\n",
    "plt.ylabel(\"Negative value\")\n",
    "plt.xlabel(\"Speech\")\n",
    "plt.title('Negativity of speeches over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most negative speeches\n",
    "The graph gives us a nice visualization of some overall trends, but it's hard to identify specific speeches here. We can just sort our dictionary, using the sorted() method we used above to look at the most negatively scored speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches[222][0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least negative speeches\n",
    "We can use the default sort (ascending values) to view the least negative speeches in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(negative, key=negative.get)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "Some of the code, descriptions, and examples above are taken from:\n",
    "* UC Berkeley's D-Lab [workshop on Text Analysis Fundamentals](https://dlab.berkeley.edu/training/text-analysis-fundamentals-unsupervised-approaches-10).\n",
    "* Software Carpentry's open source [Python curriculum](http://swcarpentry.github.io/python-novice-inflammation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go for Help\n",
    "- Contact LATISresearch@umn.edu or DASH@umn.edu\n",
    "- Software Carpentry & Programming Pizza now email-based consultations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
